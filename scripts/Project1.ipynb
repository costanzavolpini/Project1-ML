{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 \n",
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from cross_validation import *\n",
    "from pre_processing import *\n",
    "from split_jet_num import *\n",
    "from execute_code import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATAS LOADED!\n",
      "TEST DATAS LOADED!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" y: class labels\n",
    "    tx: features\n",
    "    ids: event ids \"\"\"\n",
    "y, tx, ids = load_csv_data(\"datas/train.csv\", sub_sample=False)\n",
    "print(\"TRAIN DATAS LOADED!\")\n",
    "\n",
    "no_y, tx_test, ids_test = load_csv_data(\"datas/test.csv\", sub_sample=False)\n",
    "print(\"TEST DATAS LOADED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "### Replace missing values with mean, median  or normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace missing values with mean for each feature\n",
    "# # train set\n",
    "\n",
    "# means = find_mean(tx)\n",
    "# tx_replaced_by_mean = replace_missing_values(tx, means)\n",
    "\n",
    "# # test set\n",
    "# means_test = find_mean(tx_test)\n",
    "# tx_replaced_by_mean_test = replace_missing_values(tx_test, means_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with median for each feature\n",
    "# # train set\n",
    "# medians = find_median(tx)\n",
    "# tx_replaced_by_median = replace_missing_values(tx, medians)\n",
    "\n",
    "# # test set\n",
    "# medians_test = find_median(tx_test)\n",
    "# tx_replaced_by_median_test = replace_missing_values(tx_test, medians_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace missing values with 0 and before that normalize all values without considering missing values\n",
    "# std_data_tx_with_mask = standardize(clean_array(tx))\n",
    "# tx_std_data_replaced_by_0 = replace_missing_values(std_data_tx_with_mask, np.full((30, 1), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division of the dataset looking on jet num\n",
    "If PRI_jet_num is zero or one then some features are -999.\n",
    "Divide dataset in 4 looking on jet_num 0, 1, 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_jet_0, features_jet_1, features_jet_2, features_jet_3, y_jet_0, y_jet_1, y_jet_2, y_jet_3, ids_jet_0, ids_jet_1, ids_jet_2, ids_jet_3 = generate_4_sets_looking_on_jetnum(tx, y, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set look how many missing values there are.. in order to detect how many features we want to drop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate to find which columns to drop\n",
    "# columns_to_remove_0 = columns_contains_just_missing_values(features_jet_0[0])\n",
    "# columns_to_remove_1 = columns_contains_just_missing_values(features_jet_1[0])\n",
    "# columns_to_remove_2 = columns_contains_just_missing_values(features_jet_2[0])\n",
    "# columns_to_remove_3 = columns_contains_just_missing_values(features_jet_3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed that we don't need to remove any features when jet num is 2 and 3. \n",
    "\n",
    "\n",
    "Infact: \n",
    "\n",
    "\n",
    "JET_NUM = 0 -> [4, 5, 6, 12, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "\n",
    "JET_NUM = 1 -> [4, 5, 6, 12, 26, 27, 28]\n",
    "\n",
    "\n",
    "JET_NUM = 2 -> []\n",
    "\n",
    "\n",
    "JET_NUM = 3 -> []\n",
    "\n",
    "\n",
    "We will drop features: 4, 5, 6, 12, 26, 27 and 28. And also we will drop feature 22 since it is the one of jet_num."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop feature 22 since it is the one of jet_num and it will contains the same value.\n",
    "# columns_to_remove_0.append(22)\n",
    "# columns_to_remove_1.append(22)\n",
    "# columns_to_remove_2.append(22)\n",
    "# columns_to_remove_3.append(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for constant values, if I feature contains all the same values it is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_remove_0_b = columns_contains_same_value(features_jet_0[0])\n",
    "# columns_to_remove_1_b = columns_contains_same_value(features_jet_1[0])\n",
    "# columns_to_remove_2_b = columns_contains_same_value(features_jet_2[0])\n",
    "# columns_to_remove_3_b = columns_contains_same_value(features_jet_3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that as predicted we should removed feature 22. We have found also feature 29 to remove from column 0.\n",
    "Since before we have dropped columns full of -999 (same value) we can just remove these features from each set.\n",
    "\n",
    "JET_NUM = 0 -> [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "\n",
    "\n",
    "JET_NUM = 1 -> [4, 5, 6, 12, 22, 26, 27, 28]\n",
    "\n",
    "\n",
    "JET_NUM = 2 -> [22]\n",
    "\n",
    "\n",
    "JET_NUM = 3 -> [22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove columns from subset    \n",
    "# features_dropped_0 = np.delete(features_jet_0[0], columns_to_remove_0_b, axis=1)\n",
    "# features_dropped_1 = np.delete(features_jet_1[0], columns_to_remove_1_b, axis=1)\n",
    "# features_dropped_2 = np.delete(features_jet_2[0], columns_to_remove_2_b, axis=1)\n",
    "# features_dropped_3 = np.delete(features_jet_3[0], columns_to_remove_3_b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_remove = [4, 5, 6, 12, 22, 26, 27, 28]\n",
    "\n",
    "# tx_dropped_columns = np.delete(tx, columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation \n",
    "K-fold cross-validation: original sample randomly partitioned into k equal sized subsamples.\n",
    "Repeated k times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 19\n",
    "# degree = 7\n",
    "# k_fold = 5\n",
    "\n",
    "lambdas = np.logspace(-4, 0, 30) #just for ridge regression\n",
    "\n",
    "# split data in k fold\n",
    "# k_indices = build_k_indices(y, k_fold, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ML Methods\n",
    "### Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results obtained using k = 5 and replacing missing value by mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0) Accuracy train: 0.804520 - Accuracy test: 0.799740 - Loss: 0.575760\n",
      "\n",
      "Iteration: 1) Accuracy train: 0.800310 - Accuracy test: 0.802020 - Loss: 0.585639\n",
      "\n",
      "Iteration: 2) Accuracy train: 0.806710 - Accuracy test: 0.808080 - Loss: 0.572250\n",
      "\n",
      "Iteration: 3) Accuracy train: 0.804575 - Accuracy test: 0.800720 - Loss: 0.576161\n",
      "\n",
      "Iteration: 4) Accuracy train: 0.803565 - Accuracy test: 0.804200 - Loss: 0.577485\n",
      "\n",
      "Accuracy test, mean: 0.802952, min value: 0.799740, max value: 0.808080 \n",
      "\n",
      "Accuracy train, mean: 0.803936, min value: 0.800310, max value: 0.806710 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # store the accuracy of training data and test data\n",
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "\n",
    "# for k in range(k_fold):\n",
    "#     loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y, tx_replaced_by_mean, k_indices, k, degree, least_squares)\n",
    "#     accuracy_train.append(single_accuracy_train)\n",
    "#     accuracy_test.append(single_accuracy_test)\n",
    "#     losses.append(loss)\n",
    "    \n",
    "# # Just for study the behaviour\n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results obtained using k = 5 and replacing missing value by median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0) Accuracy train: 0.805525 - Accuracy test: 0.801640 - Loss: 0.573839\n",
      "\n",
      "Iteration: 1) Accuracy train: 0.804210 - Accuracy test: 0.807480 - Loss: 0.576331\n",
      "\n",
      "Iteration: 2) Accuracy train: 0.808065 - Accuracy test: 0.809400 - Loss: 0.569957\n",
      "\n",
      "Iteration: 3) Accuracy train: 0.805485 - Accuracy test: 0.801400 - Loss: 0.574242\n",
      "\n",
      "Iteration: 4) Accuracy train: 0.804755 - Accuracy test: 0.805380 - Loss: 0.575564\n",
      "\n",
      "Accuracy test, mean: 0.805060, min value: 0.801400, max value: 0.809400 \n",
      "\n",
      "Accuracy train, mean: 0.805608, min value: 0.804210, max value: 0.808065 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # store the accuracy of training data and test data\n",
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "# weights = []\n",
    "\n",
    "# for k in range(k_fold):\n",
    "#     loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y, tx_replaced_by_median, k_indices, k, degree, least_squares)\n",
    "#     accuracy_train.append(single_accuracy_train)\n",
    "#     accuracy_test.append(single_accuracy_test)\n",
    "#     losses.append(loss)\n",
    "#     weights.append(w)\n",
    "\n",
    "# # Just for study the behaviour    \n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results obtained using k = 5 and replacing missing value by 0 after having normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "# weights = []\n",
    "\n",
    "\n",
    "# for k in range(k_fold):\n",
    "#     loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y, tx_std_data_replaced_by_0, k_indices, k, degree, least_squares)\n",
    "#     accuracy_train.append(single_accuracy_train)\n",
    "#     accuracy_test.append(single_accuracy_test)\n",
    "#     losses.append(loss)\n",
    "#     weights.append(w)\n",
    "\n",
    "# # Just for study    \n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results obtained using k = 5 and replacing missing value by mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "\n",
    "# for lambda_ in lambdas:\n",
    "#     accuracy_train_temp = []\n",
    "#     accuracy_test_temp = []\n",
    "#     losses_temp = []\n",
    "#     for k in range(k_fold):\n",
    "#         loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y, tx_replaced_by_mean, k_indices, k, degree, ridge_regression, lambda_=lambda_)\n",
    "#         accuracy_train_temp.append(single_accuracy_train)\n",
    "#         accuracy_test_temp.append(single_accuracy_test)\n",
    "#         losses_temp.append(loss)\n",
    "#     accuracy_train.append(np.mean(accuracy_train_temp))\n",
    "#     accuracy_test.append(np.mean(accuracy_test_temp))\n",
    "#     losses.append(np.mean(losses_temp))\n",
    "\n",
    "# # Just for study    \n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results obtained using k = 5 and replacing missing value by median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "\n",
    "\n",
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "# for lambda_ in lambdas:\n",
    "#     accuracy_train_temp = []\n",
    "#     accuracy_test_temp = []\n",
    "#     losses_temp = []\n",
    "#     for k in range(k_fold):\n",
    "#         loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y, tx_replaced_by_median, k_indices, k, degree, ridge_regression, lambda_=lambda_)\n",
    "#         accuracy_train_temp.append(single_accuracy_train)\n",
    "#         accuracy_test_temp.append(single_accuracy_test)\n",
    "#         losses_temp.append(loss)\n",
    "#     accuracy_train.append(np.mean(accuracy_train_temp))\n",
    "#     accuracy_test.append(np.mean(accuracy_test_temp))\n",
    "#     losses.append(np.mean(losses_temp))\n",
    "    \n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cross validation, division by jet_num with different ML implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test jet_num_0 dataset with:\n",
    "- **Least squares regression using normal equations**: <br/>\n",
    "    Accuracy test, mean: 0.583375, min value: 0.511335, max value: 0.795970 <br/>\n",
    "    Accuracy train, mean: 0.586020, min value: 0.511335, max value: 0.773929 \n",
    "- **Ridge regression using normal equations**: <br/>\n",
    "    Accuracy test, mean: 0.503224, min value: 0.476574, max value: 0.531486 <br/>\n",
    "    Accuracy train, mean: 0.503237, min value: 0.473804, max value: 0.538035 \n",
    "- **Linear regression using gradient descent**: <br/>\n",
    "- **Linear regression using stochastic gradient descent**:\n",
    "- **Logistic regression using gradient descent or SGD**:\n",
    "- **Regularized logistic regression using gradient descent or SGD**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0) Accuracy train: 0.511335 - Accuracy test: 0.513854 - Loss: 417040.188757\n",
      "\n",
      "Iteration: 1) Accuracy train: 0.773929 - Accuracy test: 0.795970 - Loss: 0.739157\n",
      "\n",
      "Iteration: 2) Accuracy train: 0.529597 - Accuracy test: 0.511335 - Loss: 95905.419265\n",
      "\n",
      "Iteration: 3) Accuracy train: 0.534635 - Accuracy test: 0.549118 - Loss: 1306.744284\n",
      "\n",
      "Iteration: 4) Accuracy train: 0.580605 - Accuracy test: 0.546599 - Loss: 21577866.290756\n",
      "\n",
      "Accuracy test, mean: 0.583375, min value: 0.511335, max value: 0.795970 \n",
      "\n",
      "Accuracy train, mean: 0.586020, min value: 0.511335, max value: 0.773929 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/costanzavolpini/Git/project1-ML/scripts/implementations.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = np.linalg.lstsq(a, b)[0]\n"
     ]
    }
   ],
   "source": [
    "# # accuracy_train = []\n",
    "# # accuracy_test = []\n",
    "# # losses = []\n",
    "# # weights = []\n",
    "\n",
    "# # for k in range(k_fold):\n",
    "# #     loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y, tx_dropped, k_indices, k, degree, least_squares)\n",
    "# #     accuracy_train.append(single_accuracy_train)\n",
    "# #     accuracy_test.append(single_accuracy_test)\n",
    "# #     losses.append(loss)\n",
    "# #     weights.append(w)\n",
    "\n",
    "    \n",
    "# # n = len(accuracy_train)\n",
    "# # for i in range(n):\n",
    "# #     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# # mean_accuracy_test = np.mean(accuracy_test)\n",
    "# # min_accuracy_test = np.min(accuracy_test)\n",
    "# # max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# # mean_accuracy_train = np.mean(accuracy_train)\n",
    "# # min_accuracy_train = np.min(accuracy_train)\n",
    "# # max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# # print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# # print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))\n",
    "\n",
    "# #LEAST SQUARE\n",
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "# weights = []\n",
    "\n",
    "# k_indices_0 = build_k_indices(y_jet_0, k_fold, seed)\n",
    "\n",
    "# for k in range(k_fold):\n",
    "#     loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y_jet_0, features_dropped_0, k_indices_0, k, degree, least_squares)\n",
    "#     accuracy_train.append(single_accuracy_train)\n",
    "#     accuracy_test.append(single_accuracy_test)\n",
    "#     losses.append(loss)\n",
    "#     weights.append(w)\n",
    "\n",
    "    \n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RIDGE REGRESSION\n",
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "\n",
    "# for lambda_ in lambdas:\n",
    "#     accuracy_train_temp = []\n",
    "#     accuracy_test_temp = []\n",
    "#     losses_temp = []\n",
    "#     for k in range(k_fold):\n",
    "#         loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y_jet_0, features_dropped_0, k_indices_0, k, degree, ridge_regression, lambda_=lambda_)\n",
    "#         accuracy_train_temp.append(single_accuracy_train)\n",
    "#         accuracy_test_temp.append(single_accuracy_test)\n",
    "#         losses_temp.append(loss)\n",
    "#     accuracy_train.append(np.mean(accuracy_train_temp))\n",
    "#     accuracy_test.append(np.mean(accuracy_test_temp))\n",
    "#     losses.append(np.mean(losses_temp))\n",
    "\n",
    "# # Just for study    \n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/costanzavolpini/Git/project1-ML/scripts/helpers.py:5: RuntimeWarning: overflow encountered in square\n",
      "  return 1/(len(y)) * np.sum(e**2)\n",
      "/Users/costanzavolpini/Git/project1-ML/scripts/proj1_helpers.py:30: RuntimeWarning: invalid value encountered in less_equal\n",
      "  y_pred[np.where(y_pred <= 0)] = -1\n",
      "/Users/costanzavolpini/Git/project1-ML/scripts/proj1_helpers.py:31: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred[np.where(y_pred > 0)] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0) Accuracy train: 0.000000 - Accuracy test: 0.000000 - Loss: nan\n",
      "\n",
      "Iteration: 1) Accuracy train: 0.000000 - Accuracy test: 0.000000 - Loss: nan\n",
      "\n",
      "Iteration: 2) Accuracy train: 0.000000 - Accuracy test: 0.000000 - Loss: nan\n",
      "\n",
      "Iteration: 3) Accuracy train: 0.000000 - Accuracy test: 0.000000 - Loss: nan\n",
      "\n",
      "Iteration: 4) Accuracy train: 0.000000 - Accuracy test: 0.000000 - Loss: nan\n",
      "\n",
      "Accuracy test, mean: 0.000000, min value: 0.000000, max value: 0.000000 \n",
      "\n",
      "Accuracy train, mean: 0.000000, min value: 0.000000, max value: 0.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # GRADIENT DESCEND \n",
    "# # HEREEEE! NOT WORK!\n",
    "# accuracy_train = []\n",
    "# accuracy_test = []\n",
    "# losses = []\n",
    "# weights = []\n",
    "# gamma = 0.01\n",
    "# max_iterations = 500\n",
    "\n",
    "# # CHECK SHAPES!!!!!!!!!!!!!!!!!!!!! and correct!\n",
    "# k_indices_0 = build_k_indices(y_jet_0, k_fold, seed)\n",
    "\n",
    "# initial_w = np.zeros(features_dropped_0.shape[1])\n",
    "\n",
    "# for k in range(k_fold):\n",
    "#     loss, w, single_accuracy_train, single_accuracy_test = cross_validation(y_jet_0, features_dropped_0, k_indices_0, k, degree, gradient_descent, initial_w=initial_w, max_iters=max_iterations, gamma=gamma)\n",
    "#     accuracy_train.append(single_accuracy_train)\n",
    "#     accuracy_test.append(single_accuracy_test)\n",
    "#     losses.append(loss)\n",
    "#     weights.append(w)\n",
    "\n",
    "# n = len(accuracy_train)\n",
    "# for i in range(n):\n",
    "#     print(\"Iteration: %d) Accuracy train: %f - Accuracy test: %f - Loss: %f\\n\" % (i, accuracy_train[i], accuracy_test[i], losses[i]))\n",
    "\n",
    "# mean_accuracy_test = np.mean(accuracy_test)\n",
    "# min_accuracy_test = np.min(accuracy_test)\n",
    "# max_accuracy_test = np.max(accuracy_test)\n",
    "\n",
    "# mean_accuracy_train = np.mean(accuracy_train)\n",
    "# min_accuracy_train = np.min(accuracy_train)\n",
    "# max_accuracy_train = np.max(accuracy_train)\n",
    "\n",
    "# print(\"Accuracy test, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_test, min_accuracy_test, max_accuracy_test))\n",
    "# print(\"Accuracy train, mean: %f, min value: %f, max value: %f \\n\" %(mean_accuracy_train, min_accuracy_train, max_accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prediction (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28075458,  0.03965436,  0.40651625, ...,  0.61755292,\n",
       "        -1.30266251,  0.39818214],\n",
       "       [ 1.63061547,  0.63344061,  1.05925506, ...,  0.        ,\n",
       "         0.        , -0.23179611],\n",
       "       [-0.51771811, -0.64151886, -0.37784339, ...,  0.        ,\n",
       "         0.        , -0.10829837],\n",
       "       ...,\n",
       "       [ 0.34569571, -1.22366112,  0.43998942, ...,  0.        ,\n",
       "         0.        ,  0.23416077],\n",
       "       [-0.72942585, -0.11217409, -0.50441304, ...,  0.        ,\n",
       "         0.        , -0.41106251],\n",
       "       [ 0.13958259, -1.31829705, -0.3187276 , ...,  0.        ,\n",
       "         0.        , -0.22539659]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test_predicted = []\n",
    "# test_poly = build_poly(tx_median_test, degree)\n",
    "# y_test_predicted = predict_labels(weights[0], test_poly)\n",
    "# create_csv_submission(ids_test, y_test_predicted, \"submission-7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 397), (1985,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build_k_indices(y_0, 5, 19).shape, y_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 250000\n"
     ]
    }
   ],
   "source": [
    "# Test my new file!!!!!!!!!!! DA QUI:\n",
    "y_test = np.zeros(tx_test.shape[0])\n",
    "tx_0, tx_1, tx_2, tx_3, y_0, y_1, y_2, y_3, ids_0, ids_1, ids_2, ids_3 = divide_dataset_looking_jetnum_and_remove_features(y, tx, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_0 = tx_0.copy()\n",
    "tx_1 = tx_1.copy()\n",
    "tx_2 = tx_2.copy()\n",
    "tx_3 = tx_3.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/costanzavolpini/Git/project1-ML/scripts/implementations.py:59: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = np.linalg.lstsq(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LEAST SQUARE\n",
      "\n",
      "Accuracy test, mean: 0.421429, min value: 0.290712, max value: 0.714393 \n",
      "\n",
      "Accuracy train, mean: 0.762284, min value: 0.761498, max value: 0.763963 \n",
      "\n",
      "2. RIDGE REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.743539, min value: 0.739315, max value: 0.747573 \n",
      "\n",
      "Accuracy train, mean: 0.760019, min value: 0.759484, max value: 0.761135 \n",
      "\n",
      "3. GRADIENT DESCENT\n",
      "\n",
      "Accuracy test, mean: 0.566150, min value: 0.533780, max value: 0.604194 \n",
      "\n",
      "Accuracy train, mean: 0.558981, min value: 0.519505, max value: 0.587241 \n",
      "\n",
      "4. STOCHASTIC GRADIENT\n",
      "\n",
      "Accuracy test, mean: 0.498008, min value: 0.316785, max value: 0.684566 \n",
      "\n",
      "Accuracy train, mean: 0.510034, min value: 0.441272, max value: 0.566948 \n",
      "\n",
      "5. LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.681734, min value: 0.550295, max value: 0.748474 \n",
      "\n",
      "Accuracy train, mean: 0.662234, min value: 0.504729, max value: 0.763575 \n",
      "\n",
      "6. REGULARIZED LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.621599, min value: 0.553598, max value: 0.671154 \n",
      "\n",
      "Accuracy train, mean: 0.621177, min value: 0.556726, max value: 0.672968 \n",
      "\n",
      "Selected method  1. LEAST SQUARE\n",
      "with accuracy 0.762284\n"
     ]
    }
   ],
   "source": [
    "acc0, m0, w0 = execute_all_methods(y_0, tx_0, ids_0, True, lambda_=0.0001, initial_w=None, max_iters=1000, gamma=1e-3)\n",
    "print(\"Selected method \", m0)\n",
    "print(\"with accuracy %f\" %(acc0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/costanzavolpini/Git/project1-ML/scripts/implementations.py:59: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = np.linalg.lstsq(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LEAST SQUARE\n",
      "\n",
      "Accuracy test, mean: 0.609543, min value: 0.472982, max value: 0.734653 \n",
      "\n",
      "Accuracy train, mean: 0.741604, min value: 0.740940, max value: 0.743052 \n",
      "\n",
      "2. RIDGE REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.728566, min value: 0.727560, max value: 0.729623 \n",
      "\n",
      "Accuracy train, mean: 0.729704, min value: 0.729285, max value: 0.729962 \n",
      "\n",
      "3. GRADIENT DESCENT\n",
      "\n",
      "Accuracy test, mean: 0.551225, min value: 0.527147, max value: 0.584279 \n",
      "\n",
      "Accuracy train, mean: 0.552931, min value: 0.527373, max value: 0.584924 \n",
      "\n",
      "4. STOCHASTIC GRADIENT\n",
      "\n",
      "Accuracy test, mean: 0.507119, min value: 0.428811, max value: 0.539528 \n",
      "\n",
      "Accuracy train, mean: 0.511791, min value: 0.448656, max value: 0.540463 \n",
      "\n",
      "5. LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.648568, min value: 0.578863, max value: 0.732525 \n",
      "\n",
      "Accuracy train, mean: 0.650471, min value: 0.580201, max value: 0.735846 \n",
      "\n",
      "6. REGULARIZED LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.596337, min value: 0.585956, max value: 0.613683 \n",
      "\n",
      "Accuracy train, mean: 0.595783, min value: 0.585553, max value: 0.609379 \n",
      "\n",
      "Selected method  1. LEAST SQUARE\n",
      "with accuracy 0.741604\n"
     ]
    }
   ],
   "source": [
    "acc1, m1, w1 = execute_all_methods(y_1, tx_1, ids_1, True, lambda_=0.0001, initial_w=None, max_iters=1000, gamma=1e-3)\n",
    "print(\"Selected method \", m1)\n",
    "print(\"with accuracy %f\" %(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/costanzavolpini/Git/project1-ML/scripts/implementations.py:59: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = np.linalg.lstsq(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LEAST SQUARE\n",
      "\n",
      "Accuracy test, mean: 0.596844, min value: 0.527841, max value: 0.703424 \n",
      "\n",
      "Accuracy train, mean: 0.821871, min value: 0.820596, max value: 0.822854 \n",
      "\n",
      "2. RIDGE REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.806908, min value: 0.801390, max value: 0.813400 \n",
      "\n",
      "Accuracy train, mean: 0.809980, min value: 0.809231, max value: 0.811514 \n",
      "\n",
      "3. GRADIENT DESCENT\n",
      "\n",
      "Accuracy test, mean: 0.577985, min value: 0.547792, max value: 0.598908 \n",
      "\n",
      "Accuracy train, mean: 0.579156, min value: 0.561935, max value: 0.605186 \n",
      "\n",
      "4. STOCHASTIC GRADIENT\n",
      "\n",
      "Accuracy test, mean: 0.582094, min value: 0.557916, max value: 0.601290 \n",
      "\n",
      "Accuracy train, mean: 0.582124, min value: 0.557593, max value: 0.603970 \n",
      "\n",
      "5. LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.742749, min value: 0.727940, max value: 0.766551 \n",
      "\n",
      "Accuracy train, mean: 0.744333, min value: 0.737221, max value: 0.769702 \n",
      "\n",
      "6. REGULARIZED LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.667256, min value: 0.628983, max value: 0.687543 \n",
      "\n",
      "Accuracy train, mean: 0.665846, min value: 0.627345, max value: 0.687072 \n",
      "\n",
      "Selected method  1. LEAST SQUARE\n",
      "with accuracy 0.821871\n"
     ]
    }
   ],
   "source": [
    "acc2, m2, w2 = execute_all_methods(y_2, tx_2, ids_2, True, lambda_=0.0001, initial_w=None, max_iters=1000, gamma=1e-3)\n",
    "print(\"Selected method \", m2)\n",
    "print(\"with accuracy %f\" %(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/costanzavolpini/Git/project1-ML/scripts/implementations.py:59: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = np.linalg.lstsq(a, b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LEAST SQUARE\n",
      "\n",
      "Accuracy test, mean: 0.547292, min value: 0.432987, max value: 0.608529 \n",
      "\n",
      "Accuracy train, mean: 0.735210, min value: 0.732344, max value: 0.738324 \n",
      "\n",
      "2. RIDGE REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.709612, min value: 0.695848, max value: 0.723150 \n",
      "\n",
      "Accuracy train, mean: 0.716121, min value: 0.711361, max value: 0.720273 \n",
      "\n",
      "3. GRADIENT DESCENT\n",
      "\n",
      "Accuracy test, mean: 0.523240, min value: 0.486011, max value: 0.580099 \n",
      "\n",
      "Accuracy train, mean: 0.527065, min value: 0.489903, max value: 0.564079 \n",
      "\n",
      "4. STOCHASTIC GRADIENT\n",
      "\n",
      "Accuracy test, mean: 0.495668, min value: 0.368231, max value: 0.538132 \n",
      "\n",
      "Accuracy train, mean: 0.506126, min value: 0.396548, max value: 0.541742 \n",
      "\n",
      "5. LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.600542, min value: 0.591381, max value: 0.618682 \n",
      "\n",
      "Accuracy train, mean: 0.598218, min value: 0.586191, max value: 0.605257 \n",
      "\n",
      "6. REGULARIZED LOGISTIC REGRESSION\n",
      "\n",
      "Accuracy test, mean: 0.608213, min value: 0.578069, max value: 0.627031 \n",
      "\n",
      "Accuracy train, mean: 0.613673, min value: 0.584612, max value: 0.640907 \n",
      "\n",
      "Selected method  1. LEAST SQUARE\n",
      "with accuracy 0.735210\n"
     ]
    }
   ],
   "source": [
    "acc3, m3, w3 = execute_all_methods(y_3, tx_3, ids_3, True, lambda_=0.0001, initial_w=None, max_iters=1000, gamma=1e-3)\n",
    "print(\"Selected method \", m3)\n",
    "print(\"with accuracy %f\" %(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568238 568238\n"
     ]
    }
   ],
   "source": [
    "tx_0_t, tx_1_t, tx_2_t, tx_3_t, _, _, _, _, ids_0_t, ids_1_t, ids_2_t, ids_3_t = divide_dataset_looking_jetnum_and_remove_features(y_test, tx_test, ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission(tx_0_t, tx_1_t, tx_2_t, tx_3_t, ids_0_t, ids_1_t, ids_2_t, ids_3_t, w0, w1, w2, w3, \"submission-28-10-to-submit\", 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
